{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1375531`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('./data/sms_supervised_train.csv')\n",
    "test_df = pd.read_csv('./data/sms_test.csv')\n",
    "unlabelled_def = pd.read_csv('./data/sms_unlabelled.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, column='textPreprocessed'):\n",
    "    \"\"\"\n",
    "    Cleans input pandas DataFrame by removing NaNs and \n",
    "    tokenising 'textPreprocessed'.\n",
    "    \n",
    "    Returns:\n",
    "        - data: cleaned pandas DataFrame\n",
    "        - tokenised_instances: a Series of lists of words\n",
    "    \"\"\"\n",
    "    data = data.dropna(subset=[column]).reset_index(drop=True)\n",
    "    tokenised_instances = data[column].apply(lambda x: x.split())\n",
    "    \n",
    "    return data, tokenised_instances\n",
    "\n",
    "\n",
    "def create_vocabulary(tokenised_instances):\n",
    "    \"\"\"\n",
    "    Creates a set of unique words from the tokenised instances\n",
    "    \n",
    "    Returns:\n",
    "        - vocabulary: a sorted list of unique words \n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for instance in tokenised_instances:\n",
    "        for token in instance:\n",
    "            vocabulary.add(token)\n",
    "            \n",
    "    return sorted(vocabulary)\n",
    "\n",
    "\n",
    "def create_count_matrix(tokenised_instances, vocabulary):\n",
    "    \"\"\"\n",
    "    Creates a count matrix where the rows are the message instances, and the \n",
    "    columns are all the unique vocabulary words. Each cell in the matrix \n",
    "    represents the number of times a given word (matrix column) appeared in \n",
    "    a given message (matrix row).\n",
    "    \n",
    "    Returns:\n",
    "        - count_matrix: pandas DataFrame\n",
    "    \"\"\"\n",
    "    count_matrix = pd.DataFrame(0, index=tokenised_instances.index, columns=vocabulary)\n",
    "    \n",
    "    for index, instance in enumerate(tokenised_instances):\n",
    "        for token in instance:\n",
    "            count_matrix.at[index, token] += 1\n",
    "            \n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "def compute_priors(data, class_column='class'):\n",
    "    \"\"\"\n",
    "    Computes the prior probability P(class=c) for each class c.\n",
    "    \n",
    "    Returns:\n",
    "        - priors: a pandas Series of the respective prior probabilities \n",
    "          for each class.\n",
    "    \"\"\"\n",
    "    instances_by_class = data[class_column].value_counts()\n",
    "    total_instances = len(data)\n",
    "    priors = instances_by_class.apply(lambda x: x / total_instances) \n",
    "    \n",
    "    return priors\n",
    "\n",
    "def compute_log_likelihoods(data, count_matrix, vocabulary, class_column='class', alpha=1):\n",
    "    \"\"\"\n",
    "    Computes likelihoods P(word=w|class=c) for each word w within each class c.\n",
    "    Store as log-likelihoods to avoid underflow problems later. Additionally, \n",
    "    implements Laplace smoothing.\n",
    "    \n",
    "    Returns:\n",
    "        - log_likelihoods: a 2D dictionary which stores all the likelihood \n",
    "          values.\n",
    "    \"\"\"\n",
    "    log_likelihoods = {}\n",
    "    classes = data[class_column].unique()\n",
    "    \n",
    "    for c in classes:\n",
    "        # create a subset of count_matrix to only include instances of class c\n",
    "        class_indices = data[data[class_column] == c].index\n",
    "        count_matrix_c = count_matrix.loc[class_indices]\n",
    "        \n",
    "        # count how many times each word appears\n",
    "        word_counts = count_matrix_c.sum()\n",
    "        # count the total amount of words in class c\n",
    "        total_word_count = word_counts.sum()\n",
    "        \n",
    "        # compute and store log-likelihoods\n",
    "        log_likelihoods_c = {}\n",
    "        for word in vocabulary:\n",
    "            word_count = word_counts[word]\n",
    "            likelihood = (word_count + alpha) / (total_word_count + len(vocabulary) * alpha)\n",
    "            log_likelihoods_c[word] = np.log(likelihood)\n",
    "        \n",
    "        log_likelihoods[c] = log_likelihoods_c\n",
    "    \n",
    "    return log_likelihoods\n",
    "\n",
    "def create_count_vector(instance, vocabulary):\n",
    "    \"\"\"\n",
    "    Creates a word count vector for an instance based on the vocabulary\n",
    "    given. Essentially an instance/row of the count_matrix described above.\n",
    "    \n",
    "    Returns: \n",
    "        - count_vector: pandas Series indexed by vocabulary words, and stores \n",
    "          their respective word counts.\n",
    "    \"\"\"\n",
    "    count_vector = pd.Series(0, index=vocabulary)\n",
    "    \n",
    "    for token in instance:\n",
    "        count_vector[token] += 1\n",
    "        \n",
    "    return count_vector\n",
    "\n",
    "def compute_posteriors(count_vector, priors, log_likelihoods, vocabulary):\n",
    "    \"\"\"\n",
    "    Computes the log posterior of an instance (count_vector) for each class.\n",
    "    \n",
    "    Returns:\n",
    "        - posteriors: a dictionary which stores each class (key) and their\n",
    "          respective log-posterior probability (value).\n",
    "    \"\"\"\n",
    "    posteriors = {}\n",
    "    \n",
    "    for c in priors.keys():\n",
    "        log_posterior = np.log(priors[c])\n",
    "\n",
    "        for word in vocabulary:\n",
    "            count = count_vector[word]\n",
    "            \n",
    "            if count > 0:\n",
    "                log_posterior += count * log_likelihoods[c][word]\n",
    "                \n",
    "        posteriors[c] = log_posterior\n",
    "    \n",
    "    return posteriors\n",
    "\n",
    "\n",
    "def train_naive_bayes(data):\n",
    "    \"\"\"\n",
    "    Trains a multinomial Naive Bayes model.\n",
    "    \n",
    "    Returns;\n",
    "        - priors: a pandas Series of the respective prior probabilities for \n",
    "          each class.\n",
    "        - log_likelihoods: a 2D dictionary which stores all the likelihood \n",
    "          values.\n",
    "        - vocabulary: a sorted list of unique words.\n",
    "    \"\"\"\n",
    "    data, tokenised_instances = preprocess(data)\n",
    "    vocabulary = create_vocabulary(tokenised_instances)\n",
    "    count_matrix = create_count_matrix(tokenised_instances, vocabulary)\n",
    "    priors = compute_priors(data)\n",
    "    log_likelihoods = compute_log_likelihoods(data, count_matrix, vocabulary)\n",
    "    \n",
    "    return priors, log_likelihoods, vocabulary\n",
    "\n",
    "\n",
    "def predict_naive_bayes(data, priors, log_likelihoods, vocabulary):\n",
    "    \"\"\"\n",
    "    Predict class labels for each instance in the test data using the\n",
    "    multinomial Naive Bayes model.\n",
    "    \n",
    "    Returns:\n",
    "        - predicted_values: a list of predicted class labels. The label can \n",
    "          be NaN if an instance doesn't contain any words from the training\n",
    "          data.\n",
    "    \"\"\"\n",
    "    predicted_values = []\n",
    "    data, tokenised_instances = preprocess(data)\n",
    "    \n",
    "    for instance in tokenised_instances:\n",
    "        # only keep tokens that were in the training set\n",
    "        instance_cleaned = [token for token in instance if token in vocabulary]\n",
    "        # skip instance entirely if it doesn't contain any words from the training set\n",
    "        if not instance_cleaned:\n",
    "            predictions.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        count_vector = create_count_vector(instance)\n",
    "        posteriors = compute_posteriors(count_vector, priors, log_likelihoods, vocabulary)\n",
    "        \n",
    "        predicted_class = max(posteriors, key=posteriors.get)\n",
    "        predicted_values.append(predicted_class)\n",
    "    \n",
    "    return predicted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "train_naive_bayes(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
